{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn\n",
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Set default figure size\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (30.0, 40.0)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image and label directory\n",
    "imgs_dir = \"../cityscapes_dataset/cityscapes_samples\"\n",
    "labels_dir = \"../cityscapes_dataset/cityscapes_samples_labels\"\n",
    "\n",
    "# Set accepted labels for this project\n",
    "label_dict = {\n",
    "    \"person\": 1,\n",
    "    \"persongroup\": 2,\n",
    "    \"rider\": 3,\n",
    "    \"bicycle\": 4,\n",
    "    \"bicyclegroup\": 5,\n",
    "    \"car\": 6,\n",
    "    \"cargroup\": 7,\n",
    "    \"bus\": 8,\n",
    "    \"truck\": 9,\n",
    "    \"traffic sign\": 10,\n",
    "    \"traffic light\": 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000 images processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19000 images processed\n"
     ]
    }
   ],
   "source": [
    "from vehicle_detection.cityscape_datalist import *\n",
    "\n",
    "# Create a data list of images\n",
    "cs_data_list = CityScapeDatalist(imgs_dir, labels_dir, label_dict)\n",
    "cs_data_list.prepare_data_list()\n",
    "\n",
    "# Split data into train test and validation\n",
    "cs_data_list.split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bounding_boxes': array([[9.850e+02, 4.260e+02, 1.078e+03, 4.950e+02],\n        [8.000e+00, 6.000e+00, 2.560e+02, 3.040e+02],\n        [1.600e+01, 4.720e+02, 2.036e+03, 1.022e+03],\n        [1.840e+02, 2.000e+00, 4.730e+02, 4.230e+02],\n        [4.890e+02, 1.400e+01, 6.330e+02, 4.220e+02],\n        [9.310e+02, 2.100e+02, 1.009e+03, 3.880e+02],\n        [1.018e+03, 2.150e+02, 1.159e+03, 3.930e+02],\n        [1.157e+03, 1.000e+00, 1.423e+03, 4.020e+02],\n        [7.690e+02, 4.000e+00, 1.152e+03, 2.210e+02],\n        [1.391e+03, 1.000e+00, 1.451e+03, 4.210e+02],\n        [4.710e+02, 4.100e+01, 5.000e+02, 5.120e+02],\n        [6.000e+00, 4.470e+02, 4.040e+02, 5.570e+02],\n        [6.650e+02, 8.800e+01, 8.840e+02, 3.240e+02],\n        [1.444e+03, 1.600e+01, 2.043e+03, 3.870e+02],\n        [1.767e+03, 5.010e+02, 1.878e+03, 5.780e+02],\n        [1.217e+03, 3.850e+02, 1.703e+03, 5.440e+02],\n        [1.920e+03, 3.680e+02, 2.040e+03, 7.130e+02],\n        [5.000e+00, 3.510e+02, 2.350e+02, 4.350e+02],\n        [4.070e+02, 4.490e+02, 4.590e+02, 4.870e+02],\n        [3.760e+02, 8.480e+02, 2.047e+03, 1.023e+03],\n        [0.000e+00, 0.000e+00, 2.048e+03, 1.024e+03]], dtype=float32),\n 'img_path': '../cityscapes_dataset/cityscapes_samples\\\\dortmund\\\\dortmund_000000_000037_leftImg8bit.png',\n 'labels': array([6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 7, 6, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Plot sample images\n",
    "cs_data_list.data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vehicle_detection.cityscape_dataset import *\n",
    "from vehicle_detection.solver import *\n",
    "\n",
    "# Create a dataset using the data list you prepared\n",
    "input_dim=(300, 300)\n",
    "train_dataset = CityScapeDataset(cs_data_list.train_list, input_dim=input_dim, mode=\"train\")\n",
    "valid_dataset = CityScapeDataset(cs_data_list.valid_list, input_dim=input_dim, mode=\"test\")\n",
    "\n",
    "# Create a solver to train the model\n",
    "solver = Solver(train_dataset, valid_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape is of shape: torch.Size([64, 3, 300, 300])\n",
      "bbox_tensor is of shape torch.Size([64, 2982, 4])\n",
      "bbox_label_tensor is of shape: torch.Size([64, 2982])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "from vehicle_detection.bbox_helper import generate_prior_bboxes, loc2bbox, center2corner\n",
    "\n",
    "# Draw bounding boxes on a sample from the data loader\n",
    "mean_img = np.asarray((127, 127, 127), dtype=np.float32).reshape(3, 1, 1)\n",
    "std_img = 128.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Now I'm going to read a few data examples from the DataLoader and plot them\n",
    "    idx, (x, gt_locs, gt_labels) = next(enumerate(solver.train_loader))\n",
    "    \n",
    "    print(\"Image shape is of shape: {}\".format(x.shape))\n",
    "    print(\"bbox_tensor is of shape {}\".format(gt_locs.shape))\n",
    "    print(\"bbox_label_tensor is of shape: {}\".format(gt_labels.shape))\n",
    "\n",
    "    NUM_IMGS = 2\n",
    "    fig, ax = plt.subplots(NUM_IMGS)\n",
    "    img_sample = x[:NUM_IMGS].clone()\n",
    "    loc_sample = gt_locs[:NUM_IMGS, :, :].clone()\n",
    "    prior_bboxes = generate_prior_bboxes()\n",
    "    for i in range(1, NUM_IMGS + 1):\n",
    "        img = np.asarray(img_sample[i - 1])\n",
    "        img = (img * std_img) + mean_img\n",
    "        img = np.ascontiguousarray(img.transpose(1, 2, 0).astype('uint8'))\n",
    "        # Convert the locations to bbox\n",
    "        bbox = loc2bbox(loc_sample[i - 1], prior_bboxes)\n",
    "        # Convert bbox to corner format\n",
    "        bbox = center2corner(bbox)\n",
    "        bbox[:,[0,2]] *= input_dim[0]\n",
    "        bbox[:,[1,3]] *= input_dim[1]\n",
    "        # Draw bounding boxes on the image\n",
    "        for k, (x, y, xw, yh) in enumerate(bbox):\n",
    "            if gt_labels[i-1][k] in range(1,12):\n",
    "                rect = patches.Rectangle([x,y], xw-x, yh-y, linewidth=1, edgecolor='r',\n",
    "                                         facecolor='none')\n",
    "                ax[i-1].add_patch(rect)\n",
    "        ax[i-1].imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vehicle_detection.ssd_net import *\n",
    "from vehicle_detection.bbox_loss import *\n",
    "\n",
    "# Instantiate the model\n",
    "num_classes = len(set(label_dict.values())) + 1\n",
    "\n",
    "ssd_net = SSD(num_classes).cuda()\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = torch.optim.Adam(ssd_net.parameters(), lr=1e-3)\n",
    "\n",
    "# Initialize loss function\n",
    "loss_function = MultiboxLoss()\n",
    "\n",
    "train_bbox_losses = []\n",
    "train_class_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model if it exists\n",
    "net_state = torch.load('vehicle_detection/ssd_net')\n",
    "ssd_net.load_state_dict(net_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is epoch 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, lr is 0.0010000, iteration 400 : total loss is 6.7228909, conf loss is 2.9401553, locs loss is 3.7827353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Accuracy = 3.794765937592784, total loss = 11.254822969436646, conf loss = 8.006637573242188, locs loss = 3.248185396194458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\nThis is epoch 1\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "lr = 1e-3\n",
    "for num_epochs in [5, 20, 20, 15, 5]:\n",
    "# for num_epochs in [1]:\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    tclass_loss, tbbox_loss, vclass_loss, vbbox_loss = solver.train(ssd_net, \n",
    "                                                                    optimizer, \n",
    "                                                                    loss_function,\n",
    "                                                                    num_epochs=num_epochs,\n",
    "                                                                    print_every=400)\n",
    "    \n",
    "    train_class_losses += tclass_loss\n",
    "    train_bbox_losses += tbbox_loss\n",
    "    lr *= 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "net_state = ssd_net.state_dict()\n",
    "torch.save(net_state, 'vehicle_detection/ssd_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prior boxes\n",
    "prior_bboxes = generate_prior_bboxes()\n",
    "train_loader = solver.train_loader\n",
    "valid_loader = solver.valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting sample bounding boxes\n",
    "NUM_IMGS = 2\n",
    "fig, ax = plt.subplots(NUM_IMGS)\n",
    "mean_img = np.asarray((127, 127, 127), dtype=np.float32).reshape(3, 1, 1)\n",
    "std_img = 128.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Now I'm going to read a few data examples from the DataLoader and plot them\n",
    "    model.eval()\n",
    "    idx, (x, gt_locs, gt_labels) = next(enumerate(valid_loader))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Move to the right device\n",
    "        x = Variable(x.cuda())\n",
    "        gt_locs = Variable(gt_locs.cuda())\n",
    "        gt_labels = Variable(gt_labels.cuda())\n",
    "\n",
    "        # Predict the classes and bbox coordinates for the input.\n",
    "        confidences, locs = model.forward(x)\n",
    "\n",
    "    img_sample = x[:NUM_IMGS].clone().cpu()\n",
    "    loc_sample = locs[:NUM_IMGS, :, :].clone().cpu()\n",
    "    conf_sample = confidences[:NUM_IMGS, :, :].clone().cpu()\n",
    "    for i in range(0, NUM_IMGS):\n",
    "        img = np.asarray(img_sample[i])\n",
    "        img = (img * std_img) + mean_img\n",
    "        img = np.ascontiguousarray(img.transpose(1, 2, 0).astype('uint8'))\n",
    "        h, w, c = img.shape\n",
    "        # Convert the locations to bbox\n",
    "        bbox = loc2bbox(loc_sample[i], prior_bboxes)\n",
    "        # Convert bbox to corner format\n",
    "        bbox = center2corner(bbox)\n",
    "        bbox[:, [0, 2]] *= w\n",
    "        bbox[:, [1, 3]] *= h\n",
    "        # Apply nms on the bounding boxes\n",
    "        sel_boxes = nms_bbox(bbox, conf_sample[i])\n",
    "        # Draw bounding boxes on the image\n",
    "        for (bbox, label) in sel_boxes:\n",
    "            (x, y, xw, yh) = bbox\n",
    "            rect = patches.Rectangle([x, y], xw - x, yh - y, linewidth=1, edgecolor='r',\n",
    "                                     facecolor='none')\n",
    "            ax[i].add_patch(rect)\n",
    "        ax[i].imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vehicle_detection.util.module_util import *\n",
    "summary_layers(ssd_net.base_net, input_size=(3, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2884, -1.3171],\n",
      "         [ 0.9776,  0.4853]],\n",
      "\n",
      "        [[-1.3066,  1.0038],\n",
      "         [ 1.4060, -0.9773]]])\n",
      "tensor([[[ 1.8054, -0.0292],\n",
      "         [-0.3860, -1.1643]]])\n",
      "tensor([[[-0.2884, -1.3171],\n",
      "         [-0.3860, -1.1643]],\n",
      "\n",
      "        [[-1.3066, -0.0292],\n",
      "         [-0.3860, -1.1643]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2, 2)\n",
    "b = torch.randn(1, 2, 2)\n",
    "print(a)\n",
    "print(b)\n",
    "print(torch.min(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2884, -1.3171],\n",
      "         [-1.3066,  1.0038]],\n",
      "\n",
      "        [[ 0.9776,  0.4853],\n",
      "         [ 1.4060, -0.9773]]])\n",
      "tensor([[[ 1.8054, -0.0292]],\n",
      "\n",
      "        [[-0.3860, -1.1643]]])\n",
      "tensor([[[-0.2884, -1.3171],\n",
      "         [-1.3066, -0.0292]],\n",
      "\n",
      "        [[-0.3860, -1.1643],\n",
      "         [-0.3860, -1.1643]]])\n"
     ]
    }
   ],
   "source": [
    "a = a.permute(1, 0 ,2).contiguous()\n",
    "b = b.permute(1, 0, 2).contiguous()\n",
    "print(a)\n",
    "print(b)\n",
    "print(torch.min(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
